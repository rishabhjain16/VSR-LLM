{
  "model1_name": "Qwen",
  "model2_name": "GPT",
  "metrics_comparison": {
    "rougeL_score": {
      "model1_value": 0.7030936804558758,
      "model2_value": 0.7425175655684023,
      "difference": -0.03942388511252648,
      "percent_difference": -5.309488548240232
    },
    "word_similarity": {
      "model1_value": 0.810103126601632,
      "model2_value": 0.8379552095193723,
      "difference": -0.02785208291774033,
      "percent_difference": -3.3238152351502777
    },
    "bertscore_f1": {
      "model1_value": 0.9401327967643738,
      "model2_value": 0.9464512467384338,
      "difference": -0.006318449974060059,
      "percent_difference": -0.6675938138212689
    },
    "meteor_score": {
      "model1_value": 0.676276415861246,
      "model2_value": 0.7141436628778545,
      "difference": -0.037867247016608485,
      "percent_difference": -5.302469094805258
    },
    "cer": {
      "model1_value": 0.21115577889447237,
      "model2_value": 0.1862110552763819,
      "difference": 0.02494472361809047,
      "percent_difference": 13.39594127806564
    },
    "semantic_similarity": {
      "model1_value": 0.7000811100006104,
      "model2_value": 0.7276552319526672,
      "difference": -0.027574121952056885,
      "percent_difference": -3.78944873083116
    },
    "viseme_alignment_score": {
      "model1_value": 0.7890415278223023,
      "model2_value": 0.818936049384722,
      "difference": -0.029894521562419674,
      "percent_difference": -3.650409770687203
    },
    "rouge1_score": {
      "model1_value": 0.7052522425681484,
      "model2_value": 0.7435202691288536,
      "difference": -0.03826802656070516,
      "percent_difference": -5.14687065700871
    },
    "bertscore_recall": {
      "model1_value": 0.9400044083595276,
      "model2_value": 0.9461703300476074,
      "difference": -0.006165921688079834,
      "percent_difference": -0.6516714266203624
    },
    "wer": {
      "model1_value": 0.3147623862487361,
      "model2_value": 0.2761375126390293,
      "difference": 0.03862487360970679,
      "percent_difference": 13.987550347857933
    },
    "rouge2_score": {
      "model1_value": 0.603955734754378,
      "model2_value": 0.6456145122050169,
      "difference": -0.04165877745063884,
      "percent_difference": -6.452577608325193
    },
    "corpus_bleu_score": {
      "model1_value": 61.65232116301992,
      "model2_value": 64.34104997922962,
      "difference": -2.6887288162097036,
      "percent_difference": -4.178869970380761
    },
    "phonetic_edit_distance": {
      "model1_value": 3.0023451299520563,
      "model2_value": 2.5617061237373737,
      "difference": 0.44063900621468255,
      "percent_difference": 17.200997496614363
    },
    "semantic_wer": {
      "model1_value": 0.29991888999938965,
      "model2_value": 0.27234476804733276,
      "difference": 0.027574121952056885,
      "percent_difference": 10.12471146398692
    },
    "sentence_bleu_score": {
      "model1_value": 52.12031905420208,
      "model2_value": 55.95203483408853,
      "difference": -3.8317157798864514,
      "percent_difference": -6.848215245876983
    },
    "bertscore_precision": {
      "model1_value": 0.9404098987579346,
      "model2_value": 0.9468307495117188,
      "difference": -0.00642085075378418,
      "percent_difference": -0.678141342272145
    }
  },
  "examples_count": 1321,
  "model1_json": "checkpoints/decode/decode_Qwen2.5-VL-7B-Instruct/vsr/en/hypo-685605.json",
  "model2_json": "checkpoints/decode/decode_visual_only_qformer/vsr/en/hypo-685605.json"
}